{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0687db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce3812a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    }
   ],
   "source": [
    "load_path='D:/VA/onehandtwohand/128/105words_DSLR_results_FH/'\n",
    "CATEGORIES=np.load(load_path+'cat_105.npy', allow_pickle=True)\n",
    "IMG_SIZE=128\n",
    "cat_len=len(CATEGORIES)\n",
    "print(cat_len)\n",
    "X=np.load(load_path+'X.npy', allow_pickle=True)\n",
    "Y=np.load(load_path+'Y.npy', allow_pickle=True)\n",
    "# X = X.astype('float32')\n",
    "# X /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3e4fcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43450,)\n"
     ]
    }
   ],
   "source": [
    "Y_new=[]\n",
    "for i in range(len(X)):\n",
    "    index = (Y[i].tolist()).index(1)\n",
    "    Y_new.append(index)\n",
    "len(Y_new)\n",
    "\n",
    "Y=np.array(Y_new)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41df9c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "pass\n"
     ]
    }
   ],
   "source": [
    "print('Splitting') \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = cat_len)\n",
    "X_train, X_new, y_train, y_new = train_test_split(X_train, y_train, test_size = 0.2, random_state = cat_len)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "X_new = np.array(X_new)\n",
    "y_new = np.array(y_new)\n",
    "\n",
    "print(\"pass\")\n",
    "del X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe3ad89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = cat_len\n",
    "input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "learning_rate = 0.0001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 50\n",
    "image_size = 72  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bb20c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Image Data Augmentation')\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "# val_generator = ImageDataGenerator(rotation_range=0, zoom_range=0.2, width_shift_range=0.2,\n",
    "#     height_shift_range=0.2, shear_range=0.2, resize=(image_size, image_size,3))\n",
    "# #                                     , horizontal_flip=True, brightness_range=[0.6,1.3])\n",
    "# val_generator.fit(X_train)\n",
    "# val_generator.fit(X_new)\n",
    "# val_generator.fit(X_test)\n",
    "\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3493d496",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d603701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7038972",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b59c93de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 72 X 72\n",
      "Patch size: 6 X 6\n",
      "Patches per image: 144\n",
      "Elements per patch: 108\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x400 with 144 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "image = X_train[np.random.choice(range(X_train.shape[0]))]\n",
    "plt.imshow((image).astype(\"uint8\"))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "resized_image = tf.image.resize(\n",
    "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
    ")\n",
    "patches = Patches(patch_size)(resized_image)\n",
    "print(f\"Image size: {image_size} X {image_size}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "print(f\"Patches per image: {patches.shape[1]}\")\n",
    "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de1a07b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1456f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8fe12da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "86/86 [==============================] - 680s 8s/step - loss: 4.8991 - accuracy: 0.0159 - top-5-accuracy: 0.0707 - val_loss: 4.5219 - val_accuracy: 0.0666 - val_top-5-accuracy: 0.1906\n",
      "Epoch 2/50\n",
      "86/86 [==============================] - 673s 8s/step - loss: 4.4642 - accuracy: 0.0334 - top-5-accuracy: 0.1260 - val_loss: 4.0663 - val_accuracy: 0.1343 - val_top-5-accuracy: 0.3698\n",
      "Epoch 3/50\n",
      "86/86 [==============================] - 671s 8s/step - loss: 4.0402 - accuracy: 0.0756 - top-5-accuracy: 0.2404 - val_loss: 3.4047 - val_accuracy: 0.2490 - val_top-5-accuracy: 0.5460\n",
      "Epoch 4/50\n",
      "86/86 [==============================] - 670s 8s/step - loss: 3.5933 - accuracy: 0.1289 - top-5-accuracy: 0.3702 - val_loss: 2.8605 - val_accuracy: 0.3422 - val_top-5-accuracy: 0.6532\n",
      "Epoch 5/50\n",
      "86/86 [==============================] - 672s 8s/step - loss: 3.2241 - accuracy: 0.1861 - top-5-accuracy: 0.4718 - val_loss: 2.5109 - val_accuracy: 0.4281 - val_top-5-accuracy: 0.7391\n",
      "Epoch 6/50\n",
      "86/86 [==============================] - 674s 8s/step - loss: 2.9677 - accuracy: 0.2300 - top-5-accuracy: 0.5398 - val_loss: 2.2375 - val_accuracy: 0.4544 - val_top-5-accuracy: 0.7621\n",
      "Epoch 7/50\n",
      "86/86 [==============================] - 673s 8s/step - loss: 2.7486 - accuracy: 0.2652 - top-5-accuracy: 0.5942 - val_loss: 2.0423 - val_accuracy: 0.5168 - val_top-5-accuracy: 0.8036\n",
      "Epoch 8/50\n",
      "86/86 [==============================] - 684s 8s/step - loss: 2.5657 - accuracy: 0.3028 - top-5-accuracy: 0.6423 - val_loss: 1.8625 - val_accuracy: 0.5230 - val_top-5-accuracy: 0.8135\n",
      "Epoch 9/50\n",
      "86/86 [==============================] - 690s 8s/step - loss: 2.3979 - accuracy: 0.3353 - top-5-accuracy: 0.6821 - val_loss: 1.7521 - val_accuracy: 0.5588 - val_top-5-accuracy: 0.8385\n",
      "Epoch 10/50\n",
      "86/86 [==============================] - 672s 8s/step - loss: 2.2591 - accuracy: 0.3726 - top-5-accuracy: 0.7155 - val_loss: 1.6208 - val_accuracy: 0.5797 - val_top-5-accuracy: 0.8488\n",
      "Epoch 11/50\n",
      "86/86 [==============================] - 680s 8s/step - loss: 2.1347 - accuracy: 0.3981 - top-5-accuracy: 0.7425 - val_loss: 1.5491 - val_accuracy: 0.5855 - val_top-5-accuracy: 0.8529\n",
      "Epoch 12/50\n",
      "86/86 [==============================] - 674s 8s/step - loss: 2.0167 - accuracy: 0.4210 - top-5-accuracy: 0.7691 - val_loss: 1.4374 - val_accuracy: 0.5937 - val_top-5-accuracy: 0.8763\n",
      "Epoch 13/50\n",
      "86/86 [==============================] - 672s 8s/step - loss: 1.8957 - accuracy: 0.4521 - top-5-accuracy: 0.7959 - val_loss: 1.3252 - val_accuracy: 0.6434 - val_top-5-accuracy: 0.8989\n",
      "Epoch 14/50\n",
      "86/86 [==============================] - 676s 8s/step - loss: 1.7724 - accuracy: 0.4789 - top-5-accuracy: 0.8231 - val_loss: 1.1993 - val_accuracy: 0.6750 - val_top-5-accuracy: 0.9178\n",
      "Epoch 15/50\n",
      "86/86 [==============================] - 676s 8s/step - loss: 1.6710 - accuracy: 0.5060 - top-5-accuracy: 0.8410 - val_loss: 1.1273 - val_accuracy: 0.6808 - val_top-5-accuracy: 0.9232\n",
      "Epoch 16/50\n",
      "86/86 [==============================] - 679s 8s/step - loss: 1.5800 - accuracy: 0.5304 - top-5-accuracy: 0.8593 - val_loss: 1.0980 - val_accuracy: 0.6947 - val_top-5-accuracy: 0.9236\n",
      "Epoch 17/50\n",
      "86/86 [==============================] - 679s 8s/step - loss: 1.5007 - accuracy: 0.5475 - top-5-accuracy: 0.8746 - val_loss: 0.9939 - val_accuracy: 0.7206 - val_top-5-accuracy: 0.9334\n",
      "Epoch 18/50\n",
      "86/86 [==============================] - 672s 8s/step - loss: 1.4277 - accuracy: 0.5663 - top-5-accuracy: 0.8888 - val_loss: 0.9834 - val_accuracy: 0.7157 - val_top-5-accuracy: 0.9388\n",
      "Epoch 19/50\n",
      "86/86 [==============================] - 677s 8s/step - loss: 1.3473 - accuracy: 0.5895 - top-5-accuracy: 0.8988 - val_loss: 0.8936 - val_accuracy: 0.7440 - val_top-5-accuracy: 0.9462\n",
      "Epoch 20/50\n",
      "86/86 [==============================] - 675s 8s/step - loss: 1.2820 - accuracy: 0.6097 - top-5-accuracy: 0.9088 - val_loss: 0.8565 - val_accuracy: 0.7625 - val_top-5-accuracy: 0.9552\n",
      "Epoch 21/50\n",
      "86/86 [==============================] - 672s 8s/step - loss: 1.2088 - accuracy: 0.6303 - top-5-accuracy: 0.9207 - val_loss: 0.8439 - val_accuracy: 0.7576 - val_top-5-accuracy: 0.9532\n",
      "Epoch 22/50\n",
      "86/86 [==============================] - 673s 8s/step - loss: 1.1594 - accuracy: 0.6410 - top-5-accuracy: 0.9278 - val_loss: 0.7933 - val_accuracy: 0.7707 - val_top-5-accuracy: 0.9593\n",
      "Epoch 23/50\n",
      "86/86 [==============================] - 672s 8s/step - loss: 1.1124 - accuracy: 0.6564 - top-5-accuracy: 0.9327 - val_loss: 0.7327 - val_accuracy: 0.7835 - val_top-5-accuracy: 0.9663\n",
      "Epoch 24/50\n",
      "86/86 [==============================] - 671s 8s/step - loss: 1.0374 - accuracy: 0.6746 - top-5-accuracy: 0.9411 - val_loss: 0.7860 - val_accuracy: 0.7601 - val_top-5-accuracy: 0.9626\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 680s 8s/step - loss: 1.0148 - accuracy: 0.6819 - top-5-accuracy: 0.9451 - val_loss: 0.6645 - val_accuracy: 0.7987 - val_top-5-accuracy: 0.9712\n",
      "Epoch 26/50\n",
      "86/86 [==============================] - 690s 8s/step - loss: 0.9632 - accuracy: 0.7009 - top-5-accuracy: 0.9503 - val_loss: 0.6414 - val_accuracy: 0.8106 - val_top-5-accuracy: 0.9741\n",
      "Epoch 27/50\n",
      "86/86 [==============================] - 686s 8s/step - loss: 0.9060 - accuracy: 0.7150 - top-5-accuracy: 0.9555 - val_loss: 0.6457 - val_accuracy: 0.8065 - val_top-5-accuracy: 0.9758\n",
      "Epoch 28/50\n",
      "86/86 [==============================] - 703s 8s/step - loss: 0.8794 - accuracy: 0.7240 - top-5-accuracy: 0.9586 - val_loss: 0.5835 - val_accuracy: 0.8172 - val_top-5-accuracy: 0.9803\n",
      "Epoch 29/50\n",
      "86/86 [==============================] - 710s 8s/step - loss: 0.8334 - accuracy: 0.7381 - top-5-accuracy: 0.9636 - val_loss: 0.5823 - val_accuracy: 0.8229 - val_top-5-accuracy: 0.9815\n",
      "Epoch 30/50\n",
      "86/86 [==============================] - 704s 8s/step - loss: 0.8091 - accuracy: 0.7463 - top-5-accuracy: 0.9658 - val_loss: 0.5502 - val_accuracy: 0.8357 - val_top-5-accuracy: 0.9823\n",
      "Epoch 31/50\n",
      "86/86 [==============================] - 687s 8s/step - loss: 0.7576 - accuracy: 0.7599 - top-5-accuracy: 0.9702 - val_loss: 0.5536 - val_accuracy: 0.8287 - val_top-5-accuracy: 0.9786\n",
      "Epoch 32/50\n",
      "86/86 [==============================] - 689s 8s/step - loss: 0.7178 - accuracy: 0.7747 - top-5-accuracy: 0.9724 - val_loss: 0.5074 - val_accuracy: 0.8500 - val_top-5-accuracy: 0.9840\n",
      "Epoch 33/50\n",
      "86/86 [==============================] - 681s 8s/step - loss: 0.7075 - accuracy: 0.7742 - top-5-accuracy: 0.9739 - val_loss: 0.4994 - val_accuracy: 0.8451 - val_top-5-accuracy: 0.9869\n",
      "Epoch 34/50\n",
      "86/86 [==============================] - 679s 8s/step - loss: 0.6662 - accuracy: 0.7883 - top-5-accuracy: 0.9755 - val_loss: 0.5320 - val_accuracy: 0.8340 - val_top-5-accuracy: 0.9844\n",
      "Epoch 35/50\n",
      "86/86 [==============================] - 684s 8s/step - loss: 0.6467 - accuracy: 0.7974 - top-5-accuracy: 0.9772 - val_loss: 0.4295 - val_accuracy: 0.8673 - val_top-5-accuracy: 0.9869\n",
      "Epoch 36/50\n",
      "86/86 [==============================] - 677s 8s/step - loss: 0.6249 - accuracy: 0.8047 - top-5-accuracy: 0.9787 - val_loss: 0.4326 - val_accuracy: 0.8628 - val_top-5-accuracy: 0.9881\n",
      "Epoch 37/50\n",
      "86/86 [==============================] - 684s 8s/step - loss: 0.5953 - accuracy: 0.8138 - top-5-accuracy: 0.9816 - val_loss: 0.4392 - val_accuracy: 0.8669 - val_top-5-accuracy: 0.9869\n",
      "Epoch 38/50\n",
      "86/86 [==============================] - 671s 8s/step - loss: 0.5727 - accuracy: 0.8189 - top-5-accuracy: 0.9824 - val_loss: 0.5151 - val_accuracy: 0.8463 - val_top-5-accuracy: 0.9827\n",
      "Epoch 39/50\n",
      "86/86 [==============================] - 674s 8s/step - loss: 0.5355 - accuracy: 0.8309 - top-5-accuracy: 0.9848 - val_loss: 0.4135 - val_accuracy: 0.8677 - val_top-5-accuracy: 0.9877\n",
      "Epoch 40/50\n",
      "86/86 [==============================] - 674s 8s/step - loss: 0.5234 - accuracy: 0.8327 - top-5-accuracy: 0.9865 - val_loss: 0.3885 - val_accuracy: 0.8792 - val_top-5-accuracy: 0.9906\n",
      "Epoch 41/50\n",
      "86/86 [==============================] - 671s 8s/step - loss: 0.5162 - accuracy: 0.8373 - top-5-accuracy: 0.9857 - val_loss: 0.4372 - val_accuracy: 0.8599 - val_top-5-accuracy: 0.9852\n",
      "Epoch 42/50\n",
      "86/86 [==============================] - 668s 8s/step - loss: 0.4806 - accuracy: 0.8467 - top-5-accuracy: 0.9882 - val_loss: 0.3865 - val_accuracy: 0.8784 - val_top-5-accuracy: 0.9901\n",
      "Epoch 43/50\n",
      "86/86 [==============================] - 680s 8s/step - loss: 0.4608 - accuracy: 0.8513 - top-5-accuracy: 0.9903 - val_loss: 0.3583 - val_accuracy: 0.8854 - val_top-5-accuracy: 0.9934\n",
      "Epoch 44/50\n",
      "86/86 [==============================] - 670s 8s/step - loss: 0.4642 - accuracy: 0.8544 - top-5-accuracy: 0.9871 - val_loss: 0.3459 - val_accuracy: 0.8919 - val_top-5-accuracy: 0.9942\n",
      "Epoch 45/50\n",
      "86/86 [==============================] - 670s 8s/step - loss: 0.4249 - accuracy: 0.8667 - top-5-accuracy: 0.9904 - val_loss: 0.3485 - val_accuracy: 0.8846 - val_top-5-accuracy: 0.9918\n",
      "Epoch 46/50\n",
      "86/86 [==============================] - 669s 8s/step - loss: 0.4168 - accuracy: 0.8711 - top-5-accuracy: 0.9903 - val_loss: 0.3037 - val_accuracy: 0.9100 - val_top-5-accuracy: 0.9934\n",
      "Epoch 47/50\n",
      "86/86 [==============================] - 670s 8s/step - loss: 0.4016 - accuracy: 0.8712 - top-5-accuracy: 0.9919 - val_loss: 0.3574 - val_accuracy: 0.8800 - val_top-5-accuracy: 0.9934\n",
      "Epoch 48/50\n",
      "86/86 [==============================] - 673s 8s/step - loss: 0.3861 - accuracy: 0.8771 - top-5-accuracy: 0.9917 - val_loss: 0.3081 - val_accuracy: 0.9002 - val_top-5-accuracy: 0.9934\n",
      "Epoch 49/50\n",
      "86/86 [==============================] - 672s 8s/step - loss: 0.3746 - accuracy: 0.8809 - top-5-accuracy: 0.9914 - val_loss: 0.2971 - val_accuracy: 0.9076 - val_top-5-accuracy: 0.9942\n",
      "Epoch 50/50\n",
      "86/86 [==============================] - 671s 8s/step - loss: 0.3649 - accuracy: 0.8835 - top-5-accuracy: 0.9932 - val_loss: 0.3260 - val_accuracy: 0.8973 - val_top-5-accuracy: 0.9930\n",
      "408/408 [==============================] - 7s 18ms/step - loss: 0.2874 - accuracy: 0.9141 - top-5-accuracy: 0.9938\n",
      "Test accuracy: 91.41%\n",
      "Test top 5 accuracy: 99.38%\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(model):\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = load_path+\"tmp\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=X_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "vit_classifier = create_vit_classifier()\n",
    "history = run_experiment(vit_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5bc35c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving\n",
      "saved\n"
     ]
    }
   ],
   "source": [
    "print('Saving')\n",
    "np.save(load_path+'_history.npy',history.history)\n",
    "# model.save(load_path+'_model.h5') \n",
    "print(\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e532541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " data_augmentation (Sequential)  (None, 72, 72, 3)   7           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " patches_3 (Patches)            (None, None, 108)    0           ['data_augmentation[0][0]']      \n",
      "                                                                                                  \n",
      " patch_encoder_1 (PatchEncoder)  (None, 144, 64)     16192       ['patches_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_17 (LayerN  (None, 144, 64)     128         ['patch_encoder_1[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_8 (MultiH  (None, 144, 64)     66368       ['layer_normalization_17[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 144, 64)      0           ['multi_head_attention_8[0][0]', \n",
      "                                                                  'patch_encoder_1[0][0]']        \n",
      "                                                                                                  \n",
      " layer_normalization_18 (LayerN  (None, 144, 64)     128         ['add_16[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 144, 128)     8320        ['layer_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 144, 128)     0           ['dense_21[0][0]']               \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 144, 64)      8256        ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 144, 64)      0           ['dense_22[0][0]']               \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 144, 64)      0           ['dropout_20[0][0]',             \n",
      "                                                                  'add_16[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_19 (LayerN  (None, 144, 64)     128         ['add_17[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_9 (MultiH  (None, 144, 64)     66368       ['layer_normalization_19[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " add_18 (Add)                   (None, 144, 64)      0           ['multi_head_attention_9[0][0]', \n",
      "                                                                  'add_17[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_20 (LayerN  (None, 144, 64)     128         ['add_18[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 144, 128)     8320        ['layer_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 144, 128)     0           ['dense_23[0][0]']               \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 144, 64)      8256        ['dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 144, 64)      0           ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " add_19 (Add)                   (None, 144, 64)      0           ['dropout_22[0][0]',             \n",
      "                                                                  'add_18[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_21 (LayerN  (None, 144, 64)     128         ['add_19[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_10 (Multi  (None, 144, 64)     66368       ['layer_normalization_21[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " add_20 (Add)                   (None, 144, 64)      0           ['multi_head_attention_10[0][0]',\n",
      "                                                                  'add_19[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_22 (LayerN  (None, 144, 64)     128         ['add_20[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 144, 128)     8320        ['layer_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 144, 128)     0           ['dense_25[0][0]']               \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 144, 64)      8256        ['dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 144, 64)      0           ['dense_26[0][0]']               \n",
      "                                                                                                  \n",
      " add_21 (Add)                   (None, 144, 64)      0           ['dropout_24[0][0]',             \n",
      "                                                                  'add_20[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_23 (LayerN  (None, 144, 64)     128         ['add_21[0][0]']                 \n",
      " ormalization)                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " multi_head_attention_11 (Multi  (None, 144, 64)     66368       ['layer_normalization_23[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " add_22 (Add)                   (None, 144, 64)      0           ['multi_head_attention_11[0][0]',\n",
      "                                                                  'add_21[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_24 (LayerN  (None, 144, 64)     128         ['add_22[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 144, 128)     8320        ['layer_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_25 (Dropout)           (None, 144, 128)     0           ['dense_27[0][0]']               \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 144, 64)      8256        ['dropout_25[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_26 (Dropout)           (None, 144, 64)      0           ['dense_28[0][0]']               \n",
      "                                                                                                  \n",
      " add_23 (Add)                   (None, 144, 64)      0           ['dropout_26[0][0]',             \n",
      "                                                                  'add_22[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_25 (LayerN  (None, 144, 64)     128         ['add_23[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_12 (Multi  (None, 144, 64)     66368       ['layer_normalization_25[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " add_24 (Add)                   (None, 144, 64)      0           ['multi_head_attention_12[0][0]',\n",
      "                                                                  'add_23[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_26 (LayerN  (None, 144, 64)     128         ['add_24[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_29 (Dense)               (None, 144, 128)     8320        ['layer_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_27 (Dropout)           (None, 144, 128)     0           ['dense_29[0][0]']               \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 144, 64)      8256        ['dropout_27[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_28 (Dropout)           (None, 144, 64)      0           ['dense_30[0][0]']               \n",
      "                                                                                                  \n",
      " add_25 (Add)                   (None, 144, 64)      0           ['dropout_28[0][0]',             \n",
      "                                                                  'add_24[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_27 (LayerN  (None, 144, 64)     128         ['add_25[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_13 (Multi  (None, 144, 64)     66368       ['layer_normalization_27[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " add_26 (Add)                   (None, 144, 64)      0           ['multi_head_attention_13[0][0]',\n",
      "                                                                  'add_25[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_28 (LayerN  (None, 144, 64)     128         ['add_26[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 144, 128)     8320        ['layer_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_29 (Dropout)           (None, 144, 128)     0           ['dense_31[0][0]']               \n",
      "                                                                                                  \n",
      " dense_32 (Dense)               (None, 144, 64)      8256        ['dropout_29[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_30 (Dropout)           (None, 144, 64)      0           ['dense_32[0][0]']               \n",
      "                                                                                                  \n",
      " add_27 (Add)                   (None, 144, 64)      0           ['dropout_30[0][0]',             \n",
      "                                                                  'add_26[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_29 (LayerN  (None, 144, 64)     128         ['add_27[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_14 (Multi  (None, 144, 64)     66368       ['layer_normalization_29[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " add_28 (Add)                   (None, 144, 64)      0           ['multi_head_attention_14[0][0]',\n",
      "                                                                  'add_27[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_30 (LayerN  (None, 144, 64)     128         ['add_28[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_33 (Dense)               (None, 144, 128)     8320        ['layer_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_31 (Dropout)           (None, 144, 128)     0           ['dense_33[0][0]']               \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_34 (Dense)               (None, 144, 64)      8256        ['dropout_31[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_32 (Dropout)           (None, 144, 64)      0           ['dense_34[0][0]']               \n",
      "                                                                                                  \n",
      " add_29 (Add)                   (None, 144, 64)      0           ['dropout_32[0][0]',             \n",
      "                                                                  'add_28[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_31 (LayerN  (None, 144, 64)     128         ['add_29[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_15 (Multi  (None, 144, 64)     66368       ['layer_normalization_31[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " add_30 (Add)                   (None, 144, 64)      0           ['multi_head_attention_15[0][0]',\n",
      "                                                                  'add_29[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_32 (LayerN  (None, 144, 64)     128         ['add_30[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_35 (Dense)               (None, 144, 128)     8320        ['layer_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_33 (Dropout)           (None, 144, 128)     0           ['dense_35[0][0]']               \n",
      "                                                                                                  \n",
      " dense_36 (Dense)               (None, 144, 64)      8256        ['dropout_33[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_34 (Dropout)           (None, 144, 64)      0           ['dense_36[0][0]']               \n",
      "                                                                                                  \n",
      " add_31 (Add)                   (None, 144, 64)      0           ['dropout_34[0][0]',             \n",
      "                                                                  'add_30[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_33 (LayerN  (None, 144, 64)     128         ['add_31[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 9216)         0           ['layer_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_35 (Dropout)           (None, 9216)         0           ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_37 (Dense)               (None, 2048)         18876416    ['dropout_35[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)           (None, 2048)         0           ['dense_37[0][0]']               \n",
      "                                                                                                  \n",
      " dense_38 (Dense)               (None, 1024)         2098176     ['dropout_36[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 1024)         0           ['dense_38[0][0]']               \n",
      "                                                                                                  \n",
      " dense_39 (Dense)               (None, 105)          107625      ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21,764,144\n",
      "Trainable params: 21,764,137\n",
      "Non-trainable params: 7\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vit_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf096baf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
