{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "153c77bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4f34606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 36/36 [00:00<00:00, 36105.92it/s]\n"
     ]
    }
   ],
   "source": [
    "#FOR ANIMALS\n",
    "# TRAIN_DIR = \"D:/VA/animals_new_dataset\"\n",
    "# FOR NUMBERS\n",
    "TRAIN_DIR = \"E:/VA/standard dataset/Massey categories/\"\n",
    "#TRAIN_DIR = \"E:/VA/standard dataset/ArASL_Database_54K_final/\"\n",
    "CATEGORIES = []\n",
    "for img in tqdm(os.listdir(TRAIN_DIR)):\n",
    "    CATEGORIES.append(img)\n",
    "#print(CATEGORIES)\n",
    "IMG_SIZE=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10a8b31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "print(CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7015ae52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "cat_len=len(CATEGORIES)\n",
    "print(cat_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "320cdb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_path = 'E:/VA/onehandtwohand/224_gray/massey_H/'\n",
    "load_path = 'E:/VA/onehandtwohand/128/massey_H/'\n",
    "\n",
    "#load_path = 'E:/VA/onehandtwohand/128/ArASL_H/'\n",
    "for category in CATEGORIES:\n",
    "    try: \n",
    "        os.makedirs(os.path.join(load_path, category))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aca58f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "def landmarks(img,results):\n",
    "      \n",
    "# hands\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "\n",
    "        mp_drawing.draw_landmarks(\n",
    "          img,\n",
    "          hand_landmarks,\n",
    "          mp_hands.HAND_CONNECTIONS,\n",
    "          mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "          mp_drawing_styles.get_default_hand_connections_style())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47e8aba4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 18.01it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 19.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 21.34it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 19.43it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 20.19it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 18.76it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 21.04it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 21.62it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 21.97it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 20.51it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 19.86it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 19.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 19.79it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 20.72it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 20.90it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 19.53it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 18.87it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 20.32it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 20.07it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 20.16it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 19.48it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 19.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 19.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 20.04it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 19.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 21.22it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 20.95it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 20.55it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 19.80it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 65/65 [00:03<00:00, 18.93it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 20.85it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 22.12it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 23.04it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 19.07it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 52/52 [00:02<00:00, 17.82it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 45/45 [00:02<00:00, 20.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# for getting just hands and face\n",
    "training_data = []\n",
    "with mp_hands.Hands(\n",
    "    static_image_mode=True,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5) as hands:\n",
    "    \n",
    "    for category in CATEGORIES:\n",
    "        path=TRAIN_DIR+\"/\"+category\n",
    "        path2=load_path+\"/\"+category\n",
    "#         print(path)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for c_img in tqdm(os.listdir(path)):\n",
    "            img_array = cv2.imread((path+\"/\"+c_img))\n",
    "            \n",
    "            image_height, image_width, _ = img_array.shape\n",
    "            # Convert the BGR image to RGB before processing.\n",
    "            results = hands.process(cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))\n",
    "                    \n",
    "#             \n",
    "            \n",
    "            img = np.zeros([image_height, image_width,3],dtype=np.uint8)\n",
    "            img.fill(255)\n",
    "            \n",
    "            if not results.multi_hand_landmarks:  # exclude saving no hands\n",
    "                continue\n",
    "            \n",
    "            landmarks(img,results) # from function\n",
    "            img = cv2.resize(img,(IMG_SIZE,IMG_SIZE))\n",
    "            #################code for gray only 1 line\n",
    "#             img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            ################\n",
    "            for i in range(9):\n",
    "                cv2.imwrite((path2+\"/\"+str(i)+c_img),img)\n",
    "                training_data.append([img, class_num])\n",
    "            \n",
    "            # conditions on one_hand\n",
    "#             if category in one_hand:\n",
    "#                 if  (results.left_hand_landmarks or results.right_hand_landmarks):   #include saving either hands\n",
    "#                     landmarks(img,results) # from function\n",
    "#                     img = cv2.resize(img,(IMG_SIZE,IMG_SIZE))\n",
    "#                     cv2.imwrite((path2+\"/\"+c_img),img)\n",
    "#                     training_data.append([img, class_num])\n",
    "                    \n",
    "#             # condition on two_hands        \n",
    "#             if category in two_hand:\n",
    "#                 if  (results.left_hand_landmarks and results.right_hand_landmarks):   #include both hand\n",
    "#                     landmarks(img,results) # from function\n",
    "#                     img = cv2.resize(img,(IMG_SIZE,IMG_SIZE))\n",
    "#                     cv2.imwrite((path2+\"/\"+c_img),img)\n",
    "#                     training_data.append([img, class_num])\n",
    "                \n",
    "            \n",
    "import random\n",
    "random.shuffle(training_data)\n",
    "# np.save('F:/VA/onehandtwohand/26words_DSLR_H/training1a.npy', training_data)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6119508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "X =[]\n",
    "y =[]\n",
    "for features, label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
    "#############gray\n",
    "#X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "###############\n",
    "# X = X.astype('float32')\n",
    "# X /= 255\n",
    "from keras.utils import np_utils\n",
    "Y = np_utils.to_categorical(y, cat_len)\n",
    "np.save(load_path+'X.npy', X)\n",
    "np.save(load_path+'Y.npy', Y)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "428d5566",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(load_path+'cat_massey.npy',CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2911be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# exclude situation\n",
    "one_hand=CATEGORIES\n",
    "\n",
    "from collections import Counter\n",
    "two_hand = list((Counter(CATEGORIES)-Counter(one_hand)).elements())\n",
    "#=['bull','cat','cow','crocodile','dog','elephant','horse','lion','tiger']\n",
    "\n",
    "print(two_hand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1994e90d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b89d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccd8293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
