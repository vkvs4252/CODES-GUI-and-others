{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ffe905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import keras\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.utils import np_utils\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d64c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = 'E:/VA/onehandtwohand/224/'\n",
    "save_path= 'E:/VA/onehandtwohand/128/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a3ec519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/VA/onehandtwohand/128/10words_dataset_FH/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/VA/onehandtwohand/128/12words_dataset_FH/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/VA/onehandtwohand/128/25words_DSLR_FH/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/VA/onehandtwohand/128/26words_DSLR_FH/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/VA/onehandtwohand/128/31words_DSLR_FH/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "IMG_SIZE=128\n",
    "all_categories=[]\n",
    "#get image dataset folder\n",
    "data_fold = []\n",
    "for data_folder in (os.listdir(load_path)):  # to see data_folder\n",
    "    data_fold.append(data_folder)\n",
    "# print(data_fold)\n",
    "    TRAIN_DIR=load_path+data_folder+'/'\n",
    "    save_path1= save_path+data_folder+'/'\n",
    "    print(save_path1)\n",
    "    \n",
    "#     get categories in folder\n",
    "    CATEGORIES=[]\n",
    "    training_data=[]\n",
    "    for category in (os.listdir(TRAIN_DIR)): #to see categories in data_folder\n",
    "        CATEGORIES.append(category)\n",
    "        \n",
    "    # exclude situation\n",
    "    \n",
    "    one_hand=['X.npy','Y.npy']\n",
    "    CATEGORIES = list((Counter(CATEGORIES)-Counter(one_hand)).elements())\n",
    "    cat_len=len(CATEGORIES)\n",
    "    # operate on images in folder\n",
    "    for category in tqdm(CATEGORIES):\n",
    "        all_categories.append(category)\n",
    "        path=TRAIN_DIR+\"/\"+category\n",
    "        \n",
    "        #to make folder if not exists\n",
    "        try: \n",
    "            os.makedirs(os.path.join(save_path1, category))\n",
    "        except:\n",
    "            pass\n",
    "        path2=save_path1+\"/\"+category\n",
    "        \n",
    "        # reading and writing images\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for c_img in (os.listdir(path)):\n",
    "            img_array = cv2.imread((path+\"/\"+c_img))\n",
    "            img = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
    "            cv2.imwrite((path2+\"/\"+c_img),img)\n",
    "            training_data.append([img, class_num])\n",
    "    #to generate npys        \n",
    "    random.shuffle(training_data)\n",
    "    X =[]\n",
    "    y =[]\n",
    "    for features, label in training_data:\n",
    "        X.append(features)\n",
    "        y.append(label)\n",
    "    X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "    # X = X.astype('float32')\n",
    "    # X /= 255\n",
    "    \n",
    "    Y = np_utils.to_categorical(y, cat_len)\n",
    "    np.save(save_path1+'X.npy', X)\n",
    "    np.save(save_path1+'Y.npy', Y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "653b02a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    }
   ],
   "source": [
    "one_hand=['X.npy','Y.npy']\n",
    "\n",
    "all_categories = list((Counter(all_categories)-Counter(one_hand)).elements())\n",
    "np.save(save_path+'/105words_DSLR_results/'+'cat_105.npy', all_categories)\n",
    "cat_len=len(all_categories)\n",
    "print(cat_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95433a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'bull', 'camel', 'cat', 'cow', 'crocodile', 'deer', 'dog', 'elephant', 'goat', 'horse', 'lion', 'tiger', 'antelope', 'bag', 'book', 'bottle', 'colour', 'dolphin', 'dupatta', 'fast', 'fish', 'frog', 'gun', 'hair', 'help', 'idea', 'internet', 'jeans', 'location', 'owl', 'pen', 'photo', 'rain', 'saree', 'school', 'shoot', 'sun', 'technology', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'accept', 'age', 'assistant', 'black', 'boot', 'card', 'cash', 'chicken', 'dance', 'earpods', 'handkerchief', 'home', 'human', 'keep', 'laptop', 'meet', 'mobile', 'monkey', 'namastey', 'person', 'proof', 'signature', 'sleep', 'spider', 'stand', 'star', 'tv', 'umbrella', 'white', 'wolf', 'yellow']\n"
     ]
    }
   ],
   "source": [
    "print(all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565b46fe",
   "metadata": {},
   "source": [
    "# create n word dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb66768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FH hands 10 words\n",
    "\n",
    "Xn=np.load('E:/VA/onehandtwohand/128/10words_dataset_FH/X.npy', allow_pickle=True)\n",
    "Yn=np.load('E:/VA/onehandtwohand/128/10words_dataset_FH/Y.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ca11de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FH hands 12 words\n",
    "\n",
    "Xa=np.load('E:/VA/onehandtwohand/128/12words_dataset_FH/X.npy', allow_pickle=True)\n",
    "Ya=np.load('E:/VA/onehandtwohand/128/12words_dataset_FH/Y.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ca720a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FH hands 25 words\n",
    "\n",
    "Xh=np.load('E:/VA/onehandtwohand/128/25words_DSLR_FH/X.npy', allow_pickle=True)\n",
    "Yh=np.load('E:/VA/onehandtwohand/128/25words_DSLR_FH/Y.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "659f5dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FH hands 26 words\n",
    "\n",
    "Xfh=np.load('E:/VA/onehandtwohand/128/26words_DSLR_FH/X.npy', allow_pickle=True)\n",
    "Yfh=np.load('E:/VA/onehandtwohand/128/26words_DSLR_FH/Y.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5538552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FH hands 31 words\n",
    "\n",
    "\n",
    "Xpfh=np.load('E:/VA/onehandtwohand/128/31words_DSLR_FH/X.npy', allow_pickle=True)\n",
    "Ypfh=np.load('E:/VA/onehandtwohand/128/31words_DSLR_FH/Y.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aca66ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b211ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "538bb710",
   "metadata": {},
   "source": [
    "# for x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e22a4b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43450, 128, 128, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "####### along X\n",
    "Xn1=np.append(Xn,Xa,axis=0)\n",
    "Xn2=np.append(Xn1,Xh,axis=0)\n",
    "Xn3=np.append(Xn2,Xfh,axis=0)\n",
    "X=np.append(Xn3,Xpfh,axis=0)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "510e6b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7593, 105)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####### for numbers Y\n",
    "Y_a = np.zeros((Yn.shape[0],cat_len-10)) # to add ahead of 10 classes\n",
    "Yaa=np.append(Yn,Y_a,axis=1) # 105 classes number matrix   with Yn number of rows              \n",
    "####### for animals Y\n",
    "Y_b1 = np.zeros((Ya.shape[0],10))  # to add before of 12 classes\n",
    "Y_b2 = np.zeros((Ya.shape[0],cat_len-(10+12))) # to add ahead of 12 classes\n",
    "Ybb=np.append(Y_b1,Ya,axis=1)\n",
    "Ycc=np.append(Ybb,Y_b2,axis=1) # 105 classes animal matrix with Ya number of rows\n",
    "# final animal with number Y\n",
    "Ydd=np.append(Yaa,Ycc,axis=0)\n",
    "Ydd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dfa20e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19547, 105)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####### for merging 25 words\n",
    "Y_c1 = np.zeros((Yh.shape[0],(10+12)))  # to add before of 25 classes\n",
    "Y_c2 = np.zeros((Yh.shape[0],cat_len-(10+12+26)))    # # to add ahead of 25 classes\n",
    "Yee=np.append(Y_c1,Yh,axis=1) # column merge\n",
    "Yff=np.append(Yee,Y_c2,axis=1)# column merge\n",
    "\n",
    "# # final animal, number, 25 classes along Y\n",
    "Ygg=np.append(Ydd,Yff,axis=0) # row\n",
    "Ygg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dffa70fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### for merging 26 words\n",
    "Y_d1 = np.zeros((Yfh.shape[0],(10+12+26)))  # to add before of 26 classes\n",
    "Y_d2 = np.zeros((Yfh.shape[0],cat_len-(10+12+26+26)))    # # to add ahead of 26 classes\n",
    "Yhh=np.append(Y_d1,Yfh,axis=1) # column merge\n",
    "Yii=np.append(Yhh,Y_d2,axis=1) # column merge\n",
    "# final animal, number, 25,26 classes along Y\n",
    "Yjj=np.append(Ygg,Yii,axis=0) # row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78268907",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### for merging 31 words\n",
    "Y_e1 = np.zeros((Ypfh.shape[0],(10+12+26+26)))  # to add before of 26 classes\n",
    "Y_e2 = np.zeros((Ypfh.shape[0],cat_len-(10+12+26+26+31)))    # # to add ahead of 26 classes\n",
    "Ykk=np.append(Y_e1,Ypfh,axis=1) # column merge\n",
    "Yll=np.append(Ykk,Y_e2,axis=1) # column merge\n",
    "# final animal, number, 25,26 classes along Y\n",
    "Ymm=np.append(Yjj,Yll,axis=0) # row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c72baf",
   "metadata": {},
   "source": [
    "for Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9f9346d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43450, 105)\n"
     ]
    }
   ],
   "source": [
    "#final 105 words \n",
    "Y=Ymm\n",
    "\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7a5ad0",
   "metadata": {},
   "source": [
    "saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01b64377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43450, 128, 128, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.save(save_path+'/105words_DSLR_results/'+'X.npy', X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b552fc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(save_path+'/105words_DSLR_results/'+'Y.npy', Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd16f4ba",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
